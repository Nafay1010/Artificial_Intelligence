{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muhammad Abdul Nafay\n",
    "## 19P-0117\n",
    "### Question#01\n",
    "For  a  simple  network  architecture  of  XOR  function  with  two  inputs,  implement  a  program  in \n",
    "Python that uses gradient decent to find a weight vector that minimizes the training loss (Note: \n",
    "You canâ€™t use any machine learning library for this question)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XOR](./XOR.png \"XOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1 + np.exp(-x)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return (x * (1 - x))\n",
    "\n",
    "def set_weights_hidden(IL, HL):\n",
    "    return (np.random.uniform(size=(IL, HL)))  # Weights of hidden Layer\n",
    "\n",
    "def set_hidden_bias(HL):\n",
    "    return (np.random.uniform(size=(1, HL)))  # Hidden Bias\n",
    "\n",
    "def set_weights_output(HL, OL):\n",
    "    return (np.random.uniform(size=(HL, OL)))  # Weights of Output Layer\n",
    "\n",
    "def set_output_bias(OL):\n",
    "    return (np.random.uniform(size=(1, OL)))  # Output Bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XOR](./XOR_Perceptrons.png \"XOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of Hidden Layer:  [[0.24181254 0.47131527]\n",
      " [0.53529616 0.20323228]]Hidden Layer Biases:  [[0.03767429 0.93751328]]Weights of output Layer:  [[0.88318975]\n",
      " [0.74123217]]Output Layer Biases:  [[0.65720394]]"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "T = np.array([[0],[1],[1],[0]]) # Expected Output\n",
    "epochs = 15000\n",
    "Learning_rate = 0.1\n",
    "\n",
    "Input_Layer = 2\n",
    "Hidden_Layer = 2\n",
    "Output_Layer = 1\n",
    "\n",
    "#Random weights and bias initialization\n",
    "weights_hidden = set_weights_hidden(Input_Layer, Hidden_Layer)\n",
    "hidden_bias = set_hidden_bias(Hidden_Layer)\n",
    "weights_output = set_weights_output(Hidden_Layer, Output_Layer)\n",
    "output_bias =  set_output_bias(Output_Layer)\n",
    "\n",
    "print(\"Weights of Hidden Layer: \", weights_hidden, end = '')\n",
    "# print(*weights_hidden)\n",
    "print(\"Hidden Layer Biases: \", hidden_bias, end = '')\n",
    "# print(*hidden_bias)\n",
    "print(\"Weights of output Layer: \", weights_output, end='')\n",
    "# print(*weights_output)\n",
    "print(\"Output Layer Biases: \", output_bias, end='')\n",
    "# print(*output_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of Hidden Layer: [6.01675396 3.84705524] [5.99503349 3.84284083]\n",
      "Weights of Output Layer: [7.85675916] [-8.53974062]\n",
      "Predicted Output: [0] [1] [1] [0]\n",
      "Expected Output: [0] [1] [1] [0]\n"
     ]
    }
   ],
   "source": [
    "#Training algorithm\n",
    "for i in range(epochs):\n",
    "\t#Forward Pass\n",
    "\thidden_layer_activation = np.dot(inputs,weights_hidden)\n",
    "\thidden_layer_activation += hidden_bias\n",
    "\thidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "\toutput_layer_activation = np.dot(hidden_layer_output,weights_output)\n",
    "\toutput_layer_activation += output_bias\n",
    "\tY = sigmoid(output_layer_activation) #Predicted Output\n",
    "\t# Y = np.where(Y < 0.1, 0, 1)\n",
    "\n",
    "\t#Backward Pass\n",
    "\tpartialderivative_Y = (T - Y) * sigmoid_derivative(Y) # Error = T - Y\n",
    "\tpartialderivative_HL = partialderivative_Y.dot(weights_output.T) * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "\t#Updating Weights and Biases\n",
    "\toutput_bias +=  np.sum(partialderivative_Y, axis=0,keepdims=True) * Learning_rate\n",
    "\tweights_output +=  hidden_layer_output.T.dot(partialderivative_Y) * Learning_rate\n",
    "\thidden_bias +=  np.sum(partialderivative_HL, axis=0,keepdims=True) * Learning_rate\n",
    "\tweights_hidden +=  inputs.T.dot(partialderivative_HL) * Learning_rate\n",
    "\n",
    "Y = np.where(Y < 0.4, 0, 1)\n",
    "\n",
    "print(\"Weights of Hidden Layer: \", end='')\n",
    "print(*weights_hidden)\n",
    "print(\"Weights of Output Layer: \",  end='')\n",
    "print(*weights_output)\n",
    "\n",
    "\n",
    "print(\"Predicted Output: \", end='')\n",
    "print(*Y)\n",
    "print(\"Expected Output: \", end='')\n",
    "print(*T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2ccb58c476f33ba3e3aee7ac07234ef6b8217ef24ad64d2a7d4fed1a57c1cd2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
